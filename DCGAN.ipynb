{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IJETKd1w3Ig"
      },
      "source": [
        "# Implementation of Deep Convolutional GANs\n",
        "Reference: https://arxiv.org/pdf/1511.06434.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXX9P0stsRUx"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bRG0HQajw3Ij"
      },
      "outputs": [],
      "source": [
        "# Run the comment below only when using Google Colab\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZOafXkO8w3Ik"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oPMbj_fqw3Ik"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ImOf7QU0w3Ik"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import datetime\n",
        "import os, sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "r352KlwVw3Ik"
      },
      "outputs": [],
      "source": [
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qA7KxqCOw3Ik"
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3OA6c3ijw3Ik"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow, imsave\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qmCKS06Gw3Ik"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'DCGAN'\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "u8P2D8Fiw3Il"
      },
      "outputs": [],
      "source": [
        "IMAGE_DIM = (32, 32, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7m9KbY0Hw3Il"
      },
      "outputs": [],
      "source": [
        "def get_sample_image(G, n_noise):\n",
        "    \"\"\"\n",
        "    Saves a grid of generated digits ranging from 0 to n_classes\n",
        "    \"\"\"\n",
        "    z = torch.randn(10, n_noise).to(DEVICE)\n",
        "    # Determine the actual output shape of the generator\n",
        "    y_hat = G(z)\n",
        "    print(\"Actual output shape of G(z):\", y_hat.shape)\n",
        "\n",
        "    # Adjust reshaping and permutation based on actual shape\n",
        "    # Example: If y_hat.shape is [10, 3, 32, 32], use:\n",
        "    y_hat = y_hat.view(10, 3, 32, 32).permute(0, 2, 3, 1)\n",
        "    result = (y_hat.detach().cpu().numpy()+1)/2.\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "R62NWejXw3Il"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "        Convolutional Discriminator for MNIST\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channel=1, num_classes=1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            # 28 -> 14\n",
        "            nn.Conv2d(in_channel, 512, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 14 -> 7\n",
        "            nn.Conv2d(512, 256, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # 7 -> 4\n",
        "            nn.Conv2d(256, 128, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            #\n",
        "            nn.Conv2d(128, 128, 3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            # reshape input, 128 -> 1\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        y_ = self.conv(x)\n",
        "        y_ = y_.view(y_.size(0), -1)\n",
        "        y_ = self.fc(y_)\n",
        "        return y_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1ZZUG0ugw3Il"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        Convolutional Generator for MNIST\n",
        "    \"\"\"\n",
        "    def __init__(self, out_channel=1, input_size=100, num_classes=784):\n",
        "        super(Generator, self).__init__()\n",
        "        assert IMAGE_DIM[0] % 2**4 == 0, 'Should be divided 16'\n",
        "        self.init_dim = (IMAGE_DIM[0] // 2**4, IMAGE_DIM[1] // 2**4)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, self.init_dim[0]*self.init_dim[1]*512),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            # x2\n",
        "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            # x2\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            # x2\n",
        "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            # x2\n",
        "            nn.ConvTranspose2d(128, out_channel, 4, stride=2, padding=1, bias=False),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        y_ = self.fc(x)\n",
        "        y_ = y_.view(y_.size(0), 512, self.init_dim[0], self.init_dim[1])\n",
        "        y_ = self.conv(y_)\n",
        "        return y_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jqroJCvPw3Il"
      },
      "outputs": [],
      "source": [
        "class LABS(Dataset):\n",
        "    '''\n",
        "    LABS Dataset\n",
        "    '''\n",
        "    def __init__(self, data_path, transform=None):\n",
        "        '''\n",
        "        Args:\n",
        "            data_path (str): path to dataset\n",
        "        '''\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        self.fpaths = sorted(glob.glob(os.path.join(data_path, '*.png')))\n",
        "        gray_lst = [266, 1085, 2176, 3048, 3439, 3469, 3539, 4577, 4848, 5177, 5502, 5713, 6947, 7383, 7693, 7774, 8137, 8144]\n",
        "        for num in gray_lst:\n",
        "            file_to_remove = os.path.join(data_path, '{:05d}.jpg'.format(num))\n",
        "            if file_to_remove in self.fpaths:  # Check if the file exists before removing\n",
        "                self.fpaths.remove(file_to_remove)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.transform(Image.open(self.fpaths[idx]))\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fpaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gukJSIbjw3Il"
      },
      "outputs": [],
      "source": [
        "D = Discriminator(in_channel=IMAGE_DIM[-1]).to(DEVICE)\n",
        "G = Generator(out_channel=IMAGE_DIM[-1]).to(DEVICE)\n",
        "# D.load_state_dict('D_dc.pkl')\n",
        "# G.load_state_dict('G_dc.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GjFo3saRw3Im"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.Resize((IMAGE_DIM[0],IMAGE_DIM[1])),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=(0.5, 0.5, 0.5),\n",
        "                                std=(0.5, 0.5, 0.5))\n",
        "                               ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pgoFp-0Kw3Im"
      },
      "outputs": [],
      "source": [
        "dataset = LABS(data_path='/content/drive/MyDrive/Gen_DL Project @Sushant/Video2Frame/merged_frames', transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Wpi2cNctw3Im"
      },
      "outputs": [],
      "source": [
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SI91WJwQw3Im"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nIbWYttow3Im"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()\n",
        "D_opt = torch.optim.Adam(D.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "G_opt = torch.optim.Adam(G.parameters(), lr=0.001, betas=(0.5, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "i3Hh0ZxOw3Im"
      },
      "outputs": [],
      "source": [
        "max_epoch = 100\n",
        "step = 0\n",
        "n_critic = 1 # for training more k steps about Discriminator\n",
        "n_noise = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6fERuVjTw3Im"
      },
      "outputs": [],
      "source": [
        "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator Label to real\n",
        "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label to fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NWlsq96Jw3Im",
        "scrolled": true,
        "outputId": "a55f91da-e484-4a44-baf7-c9b8f7a9541e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0/100, Step: 0, D Loss: 1.3886, G Loss: 0.6644, Time:05:12:07\n",
            "Actual output shape of G(z): torch.Size([10, 3, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "for epoch in range(max_epoch):\n",
        "    for idx, images in enumerate(data_loader):\n",
        "        # Training Discriminator\n",
        "        x = images.to(DEVICE)\n",
        "        x_outputs = D(x)\n",
        "        D_x_loss = criterion(x_outputs, D_labels)\n",
        "\n",
        "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
        "        z_outputs = D(G(z))\n",
        "        D_z_loss = criterion(z_outputs, D_fakes)\n",
        "        D_loss = D_x_loss + D_z_loss\n",
        "\n",
        "        D.zero_grad()\n",
        "        D_loss.backward()\n",
        "        D_opt.step()\n",
        "\n",
        "        if step % n_critic == 0:\n",
        "            # Training Generator\n",
        "            z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
        "            z_outputs = D(G(z))\n",
        "            G_loss = criterion(z_outputs, D_labels)\n",
        "\n",
        "            D.zero_grad()\n",
        "            G.zero_grad()\n",
        "            G_loss.backward()\n",
        "            G_opt.step()\n",
        "\n",
        "        if step % 500 == 0:\n",
        "            dt = datetime.datetime.now().strftime('%H:%M:%S')\n",
        "            print('Epoch: {}/{}, Step: {}, D Loss: {:.4f}, G Loss: {:.4f}, Time:{}'.format(epoch, max_epoch, step, D_loss.item(), G_loss.item(), dt))\n",
        "            G.eval()\n",
        "            img = get_sample_image(G, n_noise)\n",
        "            # Create the 'samples' directory if it doesn't exist\n",
        "            if not os.path.exists('samples'):\n",
        "                os.makedirs('samples')\n",
        "            imsave('samples/{}_step{:05d}.jpg'.format(MODEL_NAME, step), img[0])\n",
        "            G.train()\n",
        "        step += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV2CZ3Rhw3In"
      },
      "source": [
        "## Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpQvdPAHw3In"
      },
      "outputs": [],
      "source": [
        "# generation to image\n",
        "G.eval()\n",
        "imshow(get_sample_image(G, n_noise)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXTer9Hgw3In"
      },
      "outputs": [],
      "source": [
        "# Real Image\n",
        "t = Image.open(dataset.fpaths[999])\n",
        "t = (transform(t).permute(1, 2, 0)+1)/2.\n",
        "imshow(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qhPPgOT_w3In"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(state, file_name='checkpoint.pth.tar'):\n",
        "    torch.save(state, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7asogvCew3In"
      },
      "outputs": [],
      "source": [
        "# Saving params.\n",
        "# torch.save(D.state_dict(), 'D_c.pkl')\n",
        "# torch.save(G.state_dict(), 'G_c.pkl')\n",
        "save_checkpoint({'epoch': epoch + 1,\n",
        "                 'D':D.state_dict(),\n",
        "                 'G':G.state_dict(),\n",
        "                 'd_optim': D_opt.state_dict(),\n",
        "                 'g_optim' : G_opt.state_dict()},\n",
        "                'dcgan.pth.tar')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}